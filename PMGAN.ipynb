{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Mover GANs & Adaptible Universal Distribution Generators\n",
    "\n",
    "Author: [Kai Londenberg](Kai.Londenberg@googlemail.com) - 2018\n",
    "\n",
    "The recent success story of WGAN models for learning deep generative models is based on a certain formulation of adversarial training which can be interpreted as the minimization of the Wasserstein Distance $ W $ , also known as Earth-Mover Distance between a parametric generated distribution $ P_{\\theta} $ and a fixed empirical distribution $ P_r $.\n",
    "\n",
    "$ P_{\\theta} $ in the context of recent generative models such as GAN and VAE variants is usually generated by sampling a continuous random vector $ Z $ from a known probability distribution, and transforming the generated samples using a trainable  neural network function $ g_\\theta $ so that $ P_\\theta = g_\\theta(Z) $ \n",
    "\n",
    "The learning objective in this context is to **learn a distribution** via minimization of some kind of distance or divergence between $ P_r $ and $ P_\\theta $. We will call this distance function $ d(P_r, P_\\theta) $\n",
    "\n",
    "[Martin Arjovsky, Soumith Chintala and Léon Bottou showed](https://arxiv.org/abs/1701.07875) that the choice of this distance function, with the most common choice being the KL-Divergence, is of paramount importance for the ability of a gradient-based optimization method to succeed in letting $ P_\\theta $ converge towards $ P_r $ as the training optimizes the objective $ \\min_{\\theta} d(P_r, P_\\theta) $\n",
    "\n",
    "In this context, they proved that the minimization of the so-called Wasserstein Distance $ W $, also known as Earth-Mover Distance is ideal, for reasons detailed below.  \n",
    "\n",
    "To introduce the Wasserstein Distance, we define $ \\prod(P_\\theta,P_r) $ as the set of all joint distributions $ \\lambda $ whose marginal distributions are $ P_\\theta $ and $ P_r $ respectively. Then the Earth-Mover or Wasserstein Distance is defined as\n",
    "\n",
    "$$\n",
    "W(P_\\theta, P_r) = \\inf_{\\lambda \\in \\prod(P_\\theta,P_r)} \\mathbb{E}_{(x,y) \\sim \\lambda}[\\parallel x-y \\parallel]\n",
    "$$\n",
    "\n",
    "This has been described as a measure of how much probability mass would need to be moved at minimum to convert one distribution to the other. Therefore the name Earth-Mover distance. See [Wasserstein GAN Read Through](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html) for a good explanation.\n",
    "\n",
    "The Wasserstein Distance is very important, given that the paper also provided proofs that under some mild conditions:\n",
    "\n",
    " * If $ P_\\theta $ is a smooth function of $ \\theta $, then $ W(P_\\theta, P_r) $ is also smooth over $ \\theta $.\n",
    " * The above is not generally true for KL-Divergence and other Divergences or distribution distance metrics.\n",
    " * $ W(P_\\theta, P_r) \\rightarrow 0 $ implies convergence in probability of $ P_\\theta$ and $ P_r $ \n",
    " *  Consequently, $ W(P_\\theta, P_r) \\rightarrow 0 $ implies $ KL(P_\\theta, P_r) \\rightarrow 0 $ and $ {KL}(P_r, P_\\theta) \\rightarrow 0 $.\n",
    " \n",
    "\n",
    "In general, the Wasserstein distance cannot be calculated exactly. So we need to approximate it, and the WGAN paper describes a way of how to achieve this with great experimental results by exploiting the so called [Kantorovich-Rubinstein duality](https://en.wikipedia.org/wiki/Wasserstein_metric#Dual_representation_of_W1) of the Wasserstein metric.\n",
    "\n",
    "This can be achieved by a slight alteration of GAN training. The subsequent training algorithm, called a WGAN has been experimentally evaluated (and further refined in subsequent papers) to great success,\n",
    "\n",
    "The planned contribution of this paper is to develop an alternative and maybe more easily applicable method to achieve the same goal, without the necessity (but with possibility) of adversarial training, with potential applicability to mixed discrete and continuous distributions (so Z may be completely or partially discrete and might even have an intractable density), and with the potential to solve the mode collapse problem of GAN training.\n",
    "\n",
    "The idea has been strongly influenced by the Paper [Wang Q Kulkarni S Verdú S: A Nearest-Neighbor Approach to Estimating Divergence between Continuous Random Vectors](https://www.princeton.edu/~verdu/nearest.neigh.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density Estimate of a Multivariate Random Variable over a Latent Manifold Probability Space with Distance Metric.\n",
    "\n",
    "The following is an extension of the approach of [D. O. Loftsgaarden, C. P. Quesenberry: A nonparametric estimate of a multivariate density function](https://projecteuclid.org/euclid.aoms/1177700079) to the notion of a latent manifold support space and smooth but potentially nonlinear distance function. \n",
    "\n",
    "Let $P_x$ be a random variable with range $ \\mathbb{R}^q $ and $ \\{ x_i\\}_{i=1}^{n} $ be a list of i.i.d, samples with $ x_i \\sim P_x $. Furthermore, let $ d: (\\mathbb{R}^q,\\mathbb{R}^q)  \\mapsto [ 0, \\infty ) $  be a smooth and differentiable function, which we will call the natural distance function and assume to be given or learnable. \n",
    "\n",
    "We assume the random variable $P_x$ to be functionally related to this natural distance function in the sense that the distance functionally relates to the probability measure of the probability space underlying $ P_x $.\n",
    "\n",
    "This relation can be specified as follows. We assume that the natural distance function internally maps two points on space $ \\mathbb{R}^q $ to a latent manifold $ \\mathbb{A} $ and then computes their distance on that space. Therefore, we introduce mapping $ m : \\mathbb{R}^q \\mapsto \\mathbb{A} $ and latent distance metric $ d_A : \\mathbb{A}x\\mathbb{A} \\mapsto [ 0, \\infty ) $ and define $ d(x_i,x_j) = d_A(m(x_i), m(x_j)) $. We also define the inverse mapping $ s : \\mathbb{A} \\mapsto \\mathbb{R}^q $ with $ m := s^{-1} $. We call $ \\mathbb{A} $ the *latent support space*.\n",
    "\n",
    "We further assume existence of measure $ \\mu(A) $ over $ A \\subseteq \\mathbb{A} $ which is partially determined by $ d_A $. We define $ \\mu_P(A) = \\frac{\\mu(A)}{Z} $ to be a normalized probability measure based on $ \\mu $, e.g. we define $ Z = \\mu(\\mathbb{A}) $. Note that $ Z $ is considered to be unknown. \n",
    "\n",
    "The triplet $ (\\mathbb{A}, \\mathbb{B}_A, \\mu_P ) $ could already form a probability space with $ \\mathbb{B}_A $ being the Borel-Sets of $ \\mathbb{A} $. \n",
    "\n",
    "Being a manifold, $ \\mathbb{A} $ can be considered [locally w-euclidean](https://topospaces.subwiki.org/wiki/Locally_Euclidean_space) with  unknown but finite dimensionality $ w $. \n",
    "\n",
    "As such, we assume that within the $ \\epsilon $ neighbourhood of any point $ a_i \\in \\mathbb{A} $, the following property holds:\n",
    "\n",
    "$\n",
    "\\forall r <= \\epsilon: A(a_i,r) := \\{ a ; d_A(a,a_i)<r \\} \\Rightarrow \\mu(A(a_i,r)) \\approxeq V_w(r) \\Leftrightarrow \\mu_P(A(a_i,r)) \\approxeq \\frac {V_w(r)}{Z}\n",
    "$\n",
    "\n",
    "where $ V_w(r) $ is the volume of a hypersphere in $ \\mathbb{R}^w $ given it's radius. Note that $ A(a_i,r) \\in \\mathbb{B}_A$ since it is an open set.\n",
    "\n",
    "That is, we assume the measure $ \\mu(A) $ to be locally approximated by the volume of a w-dimensional hypersphere around point $ a_i $ with a radius defined over the metric $ d_A $ as long as we stay within an $\\epsilon$-neighbourhood.\n",
    "\n",
    "The intuition being, that measures can be considered to be a generalization of the concept of volume in 3 dimensional euclidean space. We can therefore derive an unnormalized probability measure (intuitively *volume in our probability space*) from our metric (intuitively *distance in our probability space*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoupling Distances & Probability Mass\n",
    "\n",
    "The problem with this approach so far is, that in this probability space, the probability of an event is inherently tied to the degrees of freedom on the *support space*, which we can intuitively think of as the space of *imaginable states*. It is, among other things, not possible to have probability zero for different *imaginable states*, unless they also have a distance of zero. But if they have a distance of zero, they can hardly be considered different.\n",
    "\n",
    "So let's modify our probability measure accordingly. Let us define another function, $ b : \\mathbb{A} \\mapsto [ 0, \\infty ) $ which serves the following purpose. We can imagine $ \\mathbb{A} $ to be a potentially crumpled sheet (aka. 2D manifold in 3D-space) of imaginable latent states, then $ b(a) $ can be thought of as the thickness of this sheet at point $ a \\in \\mathbb{A} $. If we then want to calculate the probability of a subset (aka arbitrary slice) of this sheet, we need to measure the volume of this subset.\n",
    "\n",
    "We therefore define a new (hyper-)volumetric measure $ \\beta $ for the $ \\epsilon $-neighbourhood of any point $ a_i \\in \\mathbb{A} $:\n",
    "\n",
    "$\n",
    "\\forall r <= \\epsilon: A(a,r) := \\{ a_i ; d_A(a,a_i)<r \\} \\Rightarrow \\beta(A(a,r)) = \\int_{A(a,r)} b(\\hat{a}) d\\hat{a}\n",
    "$\n",
    "\n",
    "Intuitively, this corresponds to the volume of a circular slice from a sheet of variable thickness. From this we can derive a new normalized measure $ \\beta_P $ by defining $ Z_b = \\beta(\\mathbb{A}) $ and $ \\beta_P(A) = \\frac{\\beta(A)}{Z_b} $\n",
    "so that we arrive at a new probability space defined by the triplet $ ( \\mathbb{A}, \\mathbb{B}_A, \\beta_P ) $.\n",
    "\n",
    "##### Back to our sample space\n",
    "\n",
    "If we define the set of an hypersphere around point x with radius r as $ H(x,r) := \\{x : x \\in \\mathbb{R}^q \\land d(x_i,x)<r \\} $ and equate $ P_x(X \\subseteq \\mathbb{R}^q ) := \\beta_P(m(X \\subseteq \\mathbb{R}^q )) $, for any $ \\epsilon$-neighbourhood of $ x_i $ we can set:\n",
    "\n",
    "$$\n",
    "P_x(H(x_i,d(x_i,x_j)) := \\beta_P(m(H(x_i,d(x_i,x_j)))) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can furthermore define the density of $P_x$ as $$ p_x(x) = \\lim_{r \\rightarrow 0} \\frac{P_x(H(x_i,r))}{V_w(r)} = \\lim_{x_j \\rightarrow x_i} \\frac{P_x(H(x_i,d(x_i, x_j))}{V_w(r)} $$ \n",
    "\n",
    "If we sample $ \\{ x_i \\}_{i=1}^n $ i.i.d with all $ x_i \\sim P_x $ the expected number of samples with $ d(x, x_i) \\lt r \\leq\\epsilon $ around an arbitrary point x can be calculated as\n",
    "\n",
    "$$\n",
    "E[\\vert\\{ x_i : d(x, x_i)<=r\\}_{i=1}^n\\vert] = P_x(H(x,r)) \\cdot n \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the average density p_x(x) in the neighbourhood of x via the expression\n",
    "\n",
    "$$\n",
    "\\hat{p}_x(x,r) = \\frac{\\vert\\{ x_i : d(x, x_i)<=r\\}_{i=1}^n\\vert}{n \\cdot  V_w(r)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This estimate converges almost surely to the true density p_x(x) for $ n \\to \\infty $, $ r \\to 0 $ given that for every $ r > 0 $ we can pick an $ n $ large enough to make the estimation error of $ p_x$ arbitrarily unlikely to exceed an arbitrarily small threshold if the local approximation of $ P_x(H(x,r)) $ is accurate. By simultaneously letting $ r \\to 0 $ (for example with $ r = \\frac{1}{\\sqrt{n}}$ ) we drive the error introduced by the local approximation of $ P_x(H(x,r)) $ to zero as well. \n",
    "\n",
    "$$\n",
    "E[\\hat{p}_x(x,r)] =\n",
    "\\lim_{n \\to \\infty } \\frac{P_x(H(x,n^{-\\frac{1}{2}})) \\cdot n}{n \\cdot V_w(n^{-\\frac{1}{2}})} = \\lim_{r \\rightarrow 0} \\frac{P_x(H(x_i,r))}{V_w(r)} = p_x(x)\n",
    "$$\n",
    "\n",
    "It is important to note that for a given, finite number $n $ of samples, the quality of the estimate in a certain region depends on the expected number of samples falling into the $ \\epsilon $ - neighbourhood of each other. \n",
    "\n",
    "#### Varying degrees of freedom\n",
    "\n",
    "We have for now assumed $ w $ to be a positive constant, but a variant of the above approach is imaginable where $ w $ might vary smoothly or maybe even abruptly in $ \\mathbb{A} $ as long as it stays approximately constant within any $ \\epsilon $ neighbourhood of any point a (or x) with positive probability mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Distribution Summary\n",
    "\n",
    "In summarization, we have a theoretical model which allows us to nonparametrically estimate the probability density and local probability measure, given a source of samples from a vector-valued probability distribution $ P_x $ (which needs to exist, even though we might not be calculate a normalization constant) with respect to a smooth distance metric d which is defined over a latent space. We do not assume the probability density itself to be smooth.\n",
    "\n",
    "#### Notational Conventions\n",
    "\n",
    "Before we can start writing about multiple distributions, we will introduce a few **notational conventions**.\n",
    "\n",
    "The characters a, b, x, y and z if used as subscripts will not mean indices, but rather be considered labels, associating variables, function or sets into certain groups. If not used as subscripts, they are considered to be vector-valued variables. In their uppercase variants (A,B,X,Y,Z) they will refer to sets. \n",
    "\n",
    "Subscripts i, j and u will be used for indexing. The characters n and m will be used to specify sizes of sets or lengths of sequences.\n",
    "\n",
    "We will use the notation $ P_x $ or $ P_x(\\Theta_x) $ to mean a probability distribution $ P_x $, potentially adaptable by modifying a set of parameters $ \\Theta_x $ of an underlying generative model.\n",
    "\n",
    "Equivalently, we will use the notation $ d_{x}$ or $ d_{x}(\\theta_x) $ to mean the distance metric for distribution $ P_x $, potentially parameterized by $ \\theta_x $ of a distance model. We can also, for example, write, $ d_x(x_1, x_2, \\theta_x) $ if we use it to calculate the distance between points $ x_1 $ and $x_2$.\n",
    "\n",
    "In the following, the *support space* of $ P_x $ will be denoted by $ \\mathbb{A}_x^{w_x} $, with $ w_x $ meaning the dimensionality of the support space of $ P_x $. Equivalently, the sample space of $ P_x $ will be assumed to be $ \\mathbb{R}^{q_x} $ with $ q_x $ specifying the corresponding dimensionality.\n",
    "\n",
    "The function $ m_x: \\mathbb{R}^{q_x} \\mapsto \\mathbb{A}_x^{w_x} $ will mean the (potentially unknown) function mapping a point on the sample space to the latent support space, while $ s_x: \\mathbb{A}_x^{w_x} \\mapsto \\mathbb{R}^{q_x} $ will describe the inverse mapping, i.e. $ m_x = s_x^{-1} $\n",
    "\n",
    "#### Compatible & Convertible Distributions\n",
    "\n",
    "When talking about divergences and distances between distributions, we might first need to introduce some general idea of what this might mean. Distributions $ P_x $ and $ P_y $ might be defined over the same sample space (e.g. $ q_x = q_y $ ), they might share the same latent support space $ \\mathbb{A}_x = \\mathbb{A}_y $ and they might share the same distance function, in which case $ d_x = d_y $. \n",
    "\n",
    "If two distributions share sample space and distance function, we will call them **compatible distributions**. Note that this implies $ \\mathbb{A}_x = \\mathbb{A}_y $. In the context of compatible distributions, we might drop some subscripts, e.g $ d = d_x = d_y $, $ q = q_x = q_y $ and $ \\mathbb{A} = \\mathbb{A}_x = \\mathbb{A}_y $.\n",
    "\n",
    "If two distributions are not compatible, they might still be **convertible distributions**. Examples of convertible distributions can be taken from the [CycleGAN paper](https://arxiv.org/pdf/1703.10593.pdf), e.g. the distributions of horse images and zebra-images (which might be considered compatible if they share d), or even distributions of textual image descriptions and their renderings as images (which are definitely not compatible due to different sample spaces).\n",
    "\n",
    "In the case of convertible distributions, we assume their latent support spaces to be subspaces of a larger latent support space. E.g. if $P_x$ and $P_y$ are convertible, then $  \\mathbb{A}_x \\subseteq \\mathbb{A} \\land \\mathbb{A}_y \\subseteq \\mathbb{A} $ where we assume that no trivial independent solution like $ \\mathbb{A} = \\mathbb{A}_x x \\mathbb{A}_b $ is chosen, instead we should strive to maximize potential mutual information via this mapping (for example by optimizing for cycle consistency, see below).\n",
    "\n",
    "We might then be able to find a unified distance function $ d $ between elements of their sample spaces and/or find **conversion** mappings capable of converting an element from one population into a set of one or more close elements of a compatible target population. If this conversion is at least approximately invertible, we will talk about **cycle consistent convertible distributions**. \n",
    "\n",
    "If we want to make distributions $ P_x $ and $ P_y$ as similar as possible, and have an mutual information maximizing conversion mapping between $ P_x $ and $ P_z$ available, where $ P_x $ and $ P_z$ are compatible, then we can convert the problem of making $P_x$ and $P_y$ as similar as possible into the problem of making compatible distributions $ P_x $ and $P_z$ as similar as possible.\n",
    "\n",
    "#### Local & Global Distribution Adaptation\n",
    "\n",
    "When considering an adaptible distribution generator $ P_x(\\Theta_x) $, it is important to distinguish between two different ways in which the probability density at and around a point $ x_i $ could be adapted.\n",
    "\n",
    "First, we might want to update the parameters of the model in such a way as to drive away or pull in probability mass from point $ x_i $ to or from point $ x_j $ with respect to a **certain direction** $ \\nabla d(x_i, x_j) $ along the gradient of the distance metric, or if interpreted differently to move probability mass within the local $ \\epsilon$-neighbourhood on the **support space**. We will call this approach **local distribution adaptation**\n",
    "\n",
    "On the other hand, if moving probability mass between $ x_i $ and $ x_j $ which are far from each other ( e.g. not within an $ \\epsilon $-neighbourhood of each other), we might prefer not to trust gradients, and instead prefer to *pick up* probability mass from the region around one point and *put it down* around another point on the support space. We will call this approach **global distribution adaptation**.\n",
    "\n",
    "##### Smooth Distribution Generators\n",
    "\n",
    "Generative (sub-)models in the context of GANs and Variational Autoencoders are usually modeled as deterministic functions which distort a source of random noise following a well understood distribution, such as a number of independent gaussians.\n",
    "\n",
    "We will usually use a smooth and differentiable universal function approximator to distort this noise (e.g. a neural network). But there is one downside. If the noise is smooth, and the function is smooth, the output is smooth as well. In order to approximate discrete target distributions or functions well, they have to resort to extremely nonlinear behaviour at certain boundaries, which can even lie in the middle of highly probable areas within the space of underlying latent noise. \n",
    "\n",
    "These models can be made to approximate discrete distributions well, but remain smooth in nature (up to their numerical accuracy).\n",
    "\n",
    "Models based on underlying smooth noise usually resort to variants of **local distribution adaptation** (i.e. gradient updates).\n",
    "\n",
    "##### Discrete Distribution Generators\n",
    "\n",
    "If, on the other hand, the noise is exclusively discrete, the output of these models is discrete as well. Examples of generative models following this approach would be Stochastic Neural Networks, for example, Restricted Boltzmann Machines (RBMs) over Binary Units, Deep Belief Networks ( i.e. Stacked Binary RBMs) or Dropout as Variational Inference as in [Yarin Gal: Dropout as a Bayesian Approximation](https://arxiv.org/pdf/1506.02142.pdf) and it's followup publications.\n",
    "\n",
    "These models can be made to approximate continuous distributions with a very high number of potential discrete states. This can obtain a high accuracy as well. For example, if we have n binary units or neurons, we can have up to 2^n distinguishable outputs, each with an attached numerical value.\n",
    "\n",
    "Models based on underlying discrete noise can more easily resort to variants of global distribution adaptation, but most often also use local adaptation ( Normal gradient updates in MC-Dropout, (Persistent-) Contrastive Divergence. The Wake-Sleep Algorithm uses a combination of both local and global updates, which allows him to learn mixed discrete and continuous distributions.\n",
    "\n",
    "##### Mixture Distribution Generators\n",
    "\n",
    "The most popular mixture models are probably Gaussian Mixture Models ( GMMs) and their nonparametric infinite-dimensional variant Dirichlet Process Gaussian Mixture Models (DPGMM). Gaussian Mixture Models with unlimited mixture components are Universal Distribution Approximators. The only problem being, that they require a potentially enormous number of parameters and therefore don't neccessarily generalize well, unless the data itself can be well modeled by a small number of their mixture components.\n",
    "\n",
    "##### Universal Distribution Generators\n",
    "\n",
    "If the Generator is a Neural Network which distorts a source of both discrete and smooth noise, it can be considered an Universal Mixture Model, given that it can use the discrete noise to direct probability mass to arbitrary disconnected points on the output space, and use the smooth noise to vary smoothly around these points with arbitrarily shaped distribution shapes.\n",
    "\n",
    "Such a Neural Network could also resort to a weight-sharing strategy in order to keep a good balance between the ability to generalize and it's expressiveness and capacity.\n",
    "\n",
    "Without having to fix the specifics of the architecture of such a model now, it is of **crucial importance that the used learning strategy allows for both local and global distribution adaptation**.\n",
    "\n",
    "One potential way to do this, would be to embed Stochastic Neural Network Modules within a larger network. (e.g. as independent subnetworks with different learning strategy). We will try several variants of this in the Experimental Section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Wasserstein Distance Estimator\n",
    "\n",
    "We will now construct a Mini-Batch estimator for the Wasserstein distance. We will recall the definition:\n",
    "\n",
    "Define $ \\prod(P_x,P_y) $ as the set of all joint distributions $ \\lambda $ whose marginal distributions are $ P_x $ and $ P_y $ respectively. Then the Earth-Mover or Wasserstein Distance is defined as\n",
    "\n",
    "$$\n",
    "W(P_x, P_y) = \\inf_{\\lambda \\in \\prod(P_x,P_y)} E_{(x,y) \\sim \\lambda}[d(x,y)]\n",
    "$$\n",
    "\n",
    "We assume $ P_x $ and $ P_y $ to be compatible (in the sense described in the section above) and able to generate finite lists of samples with point density estimates available for both distributions at each of the samples. \n",
    "\n",
    "In the following we assume existence of a list of samples $ \\{ x_i \\}_{i=1}^n ; x_i \\sim P_x(\\Theta_x) $  and correspondingly another list of samples $ \\{ y_j \\}_{j=1}^m ; y_j \\sim P_y $ where $ P_x(\\Theta_x) $ plays the role of an adaptible distribution parametrized by $ \\Theta_x $ and $ P_y $ plays the role of a fixed target distribution.\n",
    "\n",
    "We can precompute pointwise densities and the matrix of pairwise distances between samples. After proper normalization of the pointwise densities, we can then treat the densities as discrete probability mass and [compute the exact discrete solution of the Earth Mover Distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance#Computing_the_EMD). \n",
    "\n",
    "This requires solving the so called Transportation Problem and can be done in polynomial time. In Python, there is at least one implementation available as part of the MIT Licensed [Python Optimal Transport Library by Rémi Flamary and Nicolas Courty](https://github.com/rflamary/POT) \n",
    "\n",
    "For each combination of samples $ (x_i, y_j ) $ we obtain a score $ v(i,j) $ which specifies how much probability mass would have to move from sample $ x_i $ to $ y_j $ or the other way around. \n",
    "\n",
    "We will adopt a combination of **local distribution adaptation** and **global distribution adaptation** to achieve this, depending on distance between the samples.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pytorch, see pytorch.org\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import ot # Python Optimal Transport Library ( conda install -c conda-forge pot )\n",
    "import scipy.stats as scistats\n",
    "import probtorch # Probtorch, see https://github.com/probtorch/probtorch - pip install git+https://github.com/probtorch/probtorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmgan.pmgan_experiments import TargetMixture, SimpleParametricSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_y = TargetMixture([0.3, 0.2, 0.35, 0.15], [scistats.beta(1,7), scistats.beta(20,1), scistats.norm(4, 0.7),\n",
    "                                             scistats.binom(p=0.7, n=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x98e6668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(P_y(10000).data.numpy()[:, 0], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleParametricSampler(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "    (1): Dropout(p=0.2)\n",
       "    (2): LeakyReLU(0.01)\n",
       "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (4): Dropout(p=0.2)\n",
       "    (5): LeakyReLU(0.01)\n",
       "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (7): Dropout(p=0.2)\n",
       "    (8): LeakyReLU(0.01)\n",
       "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "loss = nn.MSELoss()\n",
    "model = SimpleParametricSampler(10, 1)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# opt = Adam(model.parameters())\n",
    "# for i in range(1000):\n",
    "#     opt.zero_grad()\n",
    "#     target = P_y.forward(1000)\n",
    "#     pred = model.forward(1000)\n",
    "#     error = loss(pred, target)\n",
    "#     error.backward()\n",
    "#     opt.step()\n",
    "#     print(\"RMSE: %.5f\" % (math.sqrt(error[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x98e6588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model(10000).data.numpy()[:, 0], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = model(110)[:, 0]\n",
    "s2 = P_y(100)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmgan.pmgan_loss import WassersteinParticleMoverLoss, L2Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wloss = WassersteinParticleMoverLoss()\n",
    "wloss.set_metric(L2Distance(), Variable(torch.FloatTensor([2.0]), requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wloss.set_samples(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logd_x = np.array(wloss.log_density_xx + wloss.log_density_yx ) # Densities of samples of x and y in P_x\n",
    "logd_y = np.array(wloss.log_density_xy + wloss.log_density_yy ) # Densities of samples of x and y in P_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor_mat(dmat):\n",
    "    len1 = len(dmat)\n",
    "    len2 = len(dmat[0])\n",
    "    result = torch.FloatTensor(len1,len2)\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            result[i,j] = dmat[i][j].data[0]\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110, 210])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr = torch.cat([to_tensor_mat(wloss.distances_xx), to_tensor_mat(wloss.distances_xy)], dim=1)\n",
    "xr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 210])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yr = torch.cat([to_tensor_mat(wloss.distances_yx), to_tensor_mat(wloss.distances_yy)], dim=1)\n",
    "yr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 3.7336515e-03, 2.5345054e-05, ..., 1.2541083e-01,\n",
       "        2.2827877e-02, 5.0919056e+01],\n",
       "       [3.7336515e-03, 0.0000000e+00, 4.3742349e-03, ..., 1.7242217e-01,\n",
       "        4.5025691e-02, 5.1794827e+01],\n",
       "       [2.5345054e-05, 4.3742349e-03, 0.0000000e+00, ..., 1.2187047e-01,\n",
       "        2.1331940e-02, 5.0847229e+01],\n",
       "       ...,\n",
       "       [1.2541083e-01, 1.7242217e-01, 1.2187047e-01, ..., 0.0000000e+00,\n",
       "        4.1227240e-02, 4.5990437e+01],\n",
       "       [2.2827877e-02, 4.5025691e-02, 2.1331940e-02, ..., 4.1227240e-02,\n",
       "        0.0000000e+00, 4.8785610e+01],\n",
       "       [5.0919056e+01, 5.1794827e+01, 5.0847229e+01, ..., 4.5990437e+01,\n",
       "        4.8785610e+01, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmat = torch.cat([xr,yr],dim=0)\n",
    "dmat.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.589057922363281"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmat[120,121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "\n",
    " * [Wang Q Kulkarni S Verdú S: A Nearest-Neighbor Approach to Estimating Divergence between Continuous Random Vectors](https://www.princeton.edu/~verdu/nearest.neigh.pdf)\n",
    " * [D. O. Loftsgaarden, C. P. Quesenberry: A nonparametric estimate of a multivariate density function](https://projecteuclid.org/euclid.aoms/1177700079)\n",
    " * [Martin Arjovsky, Soumith Chintala, Léon Bottou: Wasserstein GAN](https://arxiv.org/abs/1701.07875)\n",
    " * [Wasserstein GAN Read Through](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links\n",
    "\n",
    "\n",
    " * [Wikipedia: Weak Convergence of Measures](https://en.wikipedia.org/wiki/Convergence_of_measures#Weak_convergence_of_measures)        \n",
    " * [Wikipedia: Random Variable](https://en.wikipedia.org/wiki/Random_variable)\n",
    " * [Wikipedia: Pushforward Measure](https://en.wikipedia.org/wiki/Pushforward_measure)\n",
    " * [Wikipedia: Radon Measure](https://en.wikipedia.org/wiki/Radon_measure)\n",
    " * [Wikipedia: Wasserstein Metric](https://en.wikipedia.org/wiki/Wasserstein_metric)\n",
    "                                        \n",
    "                                             \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
